
## 2. Mapping Transcripts to Cells

### File Format Overview â€“ transcripts.parquet

Xenium outputs transcript data in 3 different formats:
	1. transcripts.csv.gz
	2. transcripts.parquet
	3. transcripts.zarr.zip

Depending on the choice of downstream analysis tools, one format may be easier or faster to process than another.

This article will focus on the Apache Parquet file format. Unlike the row-based CSV format where data is stored line by line (row by row), Parquet is column-based. Because data in each column usually share the same data type, Parquet files can be compressed and decompressed very efficiently. In general, this modern file format is much smaller and faster relative to CSV. For a more in-depth comparison between Parquet and CSV, please take a look at this article.

Use the following python script to quickly determine the column names and their associated data types in transcripts.parquet. The script will also print out the first 10 records. Please run this script using the xenium conda environment.

* transcripts_parquet.py

Give files with .py extension execute permission

```bash
chmod 755 *.py
# Activate the xenium conda environment 
conda activate xenium
python ./transcripts_parquet.py
```
Running the python script returns the following, showing that transcripts.parquet contains 8 columns.

```bash
|transcript_id        |uint64 |
|cell_id              |int32  |
|overlaps_nucleus     |uint8  |
|feature_name         |object |
|x_location           |float32|
|y_location           |float32|
|z_location           |float32|
|qv                   |float32|

dtype: object
     transcript_id  cell_id  overlaps_nucleus feature_name  x_location  y_location  z_location         qv
0  281474976710656      153                 1     b'Chst9'  467.284912  723.035156   10.867549  13.371499
1  281474976710657       85                 1   b'Bhlhe40'  618.211121  717.488708   14.101116  12.122915
2  281474976710658      191                 0      b'Wfs1'  598.623840  782.937134   13.952497  40.000000
3  281474976710659      203                 0      b'Wfs1'  610.280945  798.482544   13.899248  40.000000
4  281474976710660      199                 0      b'Dkk3'  532.640747  800.270020   14.038778  40.000000
5  281474976710661      205                 0      b'Gjb2'  412.885162  813.587158   13.887715  40.000000
6  281474976710662       -1                 0      b'Dkk3'  535.707947  861.141846   14.190361  12.725308
7  281474976710663       -1                 0      b'Lyz2'  340.266418  865.291199   13.883341  40.000000
8  281474976710664       -1                 0      b'Car4'  211.336823  879.179504   13.700431  40.000000
9  281474976710665     1342                 0      b'Lyz2'  367.774048  890.858215   13.799179  40.000000
```


Here are a few quick notes about some of these columns:
* cell_id: If a transcript is not associated with a cell, it will get -1. Cell assignment is based on Xenium's default 2D segmentation mask, and is likely irrelevant if we want to use the 3D segmentation result from Cellpose.
* x_location, y_location, z_location: These 3 columns store the (x, y, z) coordinates of the transcript in microns. The origin is found in the upper-left corner of the bottom z-slice, as the following image illustrates.
* qv: Phred-scaled quality score of all decoded transcripts. It is up to the customer to determine a suitable Q-Score threshold when analyzing Xenium data. At 10x Genomics, we typically filter out transcripts with Q-Score < 20.

### Reading Cellpose Segmentation Mask
Cellpose produces a *.ome_seg.npy output file that contains the nucleus segmentation results. This file is documented on  https://cellpose.readthedocs.io/en/latest/outputs.html.
It is also possible to list all the keys, the datatypes, and dimensions using the following python script. Please run this script using the xenium conda environment.

```bash
python ./ome_seg.py
```

Running the script above would produce an output such as the following:

```bash
Key: outlines
Dtype: <class 'numpy.ndarray'>
Dimension: (12, 3125, 2861)
Key: masks
Dtype: <class 'numpy.ndarray'>
Dimension: (12, 3125, 2861)
Key: chan_choose
Dtype: <class 'list'>
Length: 2
Key: img
Dtype: <class 'numpy.ndarray'>
Dimension: (12, 3125, 2861)
Key: ismanual
Dtype: <class 'numpy.ndarray'>
Dimension: (32845,)
Key: filename
Dtype: <class 'str'>
Key: flows
Dtype: <class 'list'>
Length: 5
Key: est_diam
Dtype: <class 'float'>
```

Of these, the ndarray stored under the "masks" key will be most relevant to us, since every (z-slice, y-pixel, x-pixel) in the original image that is given to Cellpose will be assigned to a unique cell ID. If 0 is assigned, then that location is dark in the original image and is not associated with a cell based on DAPI nucleus segmentation.

### Mapping Transcripts
Putting everything together, the Python script below will perform these tasks:
	1. Process all transcripts from transcripts.parquet, and assign them to cells based on Cellpose nucleus segmentation results.
	2. Perform a neighborhood search for unassigned transcripts. Since Cellpose only performs nucleus segmentation from DAPI signals, many transcripts that are located in the cytoplasm will be in unassigned space. The script uses a heuristic that associates those transcripts with the nearest nucleus if the Euclidean distance is within a user-specified limit. The default nuclear expansion distance is 10 microns, and this can be adjusted via the -nuc_exp parameter.
	3. Generate a feature-cell matrix and output that matrix in MTX format that is compatible with Seurat and Scanpy.

Running the script with -h would display the help text that lists the required and optional parameters. This script should be run using the xenium conda environment. It's also important to specify the correct -pix_size argument depending on the level that was extracted from the image pyramid in the nucleus segmentation section.


* map_transcripts.py

then a sample invocation might look like the following:

```bash
# Activate the xenium conda environment
conda activate xenium

# Map transcripts to cells
python ./map_transcripts.py -cellpose level_2_morphology.ome_seg.npy -transcript transcripts.parquet -out feature_cell_matrix -pix_size 0.85
```

Your code has two issues that need to be fixed. I'll explain how to address both.

### Issue 1: `TypeError: decoding str is not supported`

This issue is happening because you're trying to decode a string that's already a string in this line:

```python
feature_to_index[str(val, 'utf-8')] = index
```

In Python 3, strings are already Unicode, and decoding them is not needed (or allowed). You should only decode if the object is a `bytes` type. Here's how to fix it by checking if `val` is a `bytes` object before decoding:

#### Fix for Issue 1:
Replace this line:

```python
feature_to_index[str(val, 'utf-8')] = index
```

With this:

```python
if isinstance(val, bytes):
    feature_to_index[val.decode('utf-8')] = index
else:
    feature_to_index[val] = index
```

This will ensure that `val` is only decoded if it's in `bytes`, while `str` values will be handled directly.

### Issue 2: Decoding in multiple places

The second issue occurs in multiple places where you're trying to decode `row['feature_name']` and other columns that might already be strings. Specifically, in this section of your code:

```python
feature = str(row['feature_name'], 'utf-8')
```

You're trying to decode `row['feature_name']` as if it's `bytes`, but if it is already a `str`, you get the same error. So, the fix here is similar to Issue 1.

#### Fix for Issue 2:
Replace this line:

```python
feature = str(row['feature_name'], 'utf-8')
```

With:

```python
if isinstance(row['feature_name'], bytes):
    feature = row['feature_name'].decode('utf-8')
else:
    feature = row['feature_name']
```

### Other places where similar changes should be made:

- In the `write_sparse_mtx` function, when writing features to `features.tsv`, you have:

```python
feature = str(f, 'utf-8')
```

This needs to be fixed similarly:

```python
if isinstance(f, bytes):
    feature = f.decode('utf-8')
else:
    feature = f
```

### Summary of Fixes:

- **First issue**: When creating the `feature_to_index` dictionary, decode only if the value is in `bytes`.
- **Second issue**: When working with `row['feature_name']` and similar values, decode only if they are `bytes`.

### Updated Code Example:

```python
# Handling feature_name decoding in the feature_to_index dictionary
for index, val in enumerate(features):
    if isinstance(val, bytes):
        feature_to_index[val.decode('utf-8')] = index
    else:
        feature_to_index[val] = index

# Handling decoding during transcript iteration
for index, row in transcripts_df.iterrows():
    if index % args.rep_int == 0:
        print(index, "transcripts processed.")

    if isinstance(row['feature_name'], bytes):
        feature = row['feature_name'].decode('utf-8')
    else:
        feature = row['feature_name']

    x = row['x_location']
    y = row['y_location']
    z = row['z_location']
    qv = row['qv']

    # Rest of your code...
```

Apply this logic wherever you handle `feature_name` or similar objects, and it will ensure that your code runs smoothly. 

It starts running after I corrected the code! :)

Completed!

The script spends the majority of time performing neighborhood searches, so the choice of -nuc_exp will greatly impact overall runtime. Larger nuclear expansion distances will allow more transcripts to be associated with a cell, but we expect a roughly cubic growth in runtime since neighborhoods expand in 3 dimensions.

The table below shows the relationship between nuclear expansion distance, cell-associated Q20 transcripts, and script runtime. These results are based on processing the first million transcripts, 806,508 of which have Q-Score >= 20, from a mouse brain dataset. Depending on the nuclear expansion distance, different numbers of transcripts can be associated with a cell (cell-associated Q20 transcript) as shown in the table below. The whole dataset has almost 20 million decoded transcripts, so we can expect a twenty-fold increase in runtime to map all transcripts in this mouse brain sample.

Nuclear Expansion Distance	Cell-Associated Q20 Transcripts	Script Runtime
0 micron	49,846	2 min. 23 sec.
5 microns	98,031	12 min. 24 sec.
10 microns	125,874	60 min. 14 sec.
15 microns	139,917	165 min. 7 sec.

### Downstream Analysis
After running the Python transcript mapping script, the resulting feature-cell matrix can be read by popular third-party tools such as Seurat and Scanpy.
To demonstrate compatibility, we were able to follow this Seurat vignette and use the Read10X() function to load the matrix. We produced the following plots, and it should be straightforward to cluster the cells, generate a UMAP projection, and find differentially expressed features.

Likewise, we were able to follow this Scanpy tutorial and use the read_10x_mtx() function to load the matrix. The following plot shows the 20 genes that have the highest fraction of counts in each cell, across all cells, and it should be straightforward to proceed further with the tutorial and run the standard suite of single cell data analysis with this feature-cell matrix.

If you use Cellpose in your publication, please cite Cellpose based on citation guidelines here.

References:
https://www.10xgenomics.com/analysis-guides/performing-3d-nucleus-segmentation-with-cellpose-and-generating-a-feature-cell-matrix#introduction